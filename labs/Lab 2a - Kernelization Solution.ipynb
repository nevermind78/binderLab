{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lab 2a: Tuning Support Vector Machines\n",
    "Support Vector Machines are powerful methods, but they also require careful tuning. We'll explore SVM kernels and hyperparameters on an artificial dataset. We'll especially look at model underfitting and overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# General imports\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import openml as oml\n",
    "from matplotlib import cm\n",
    "import sys\n",
    "import os\n",
    "# Hide convergence warning for now\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# Hiding all warnings. Not recommended, just for compilation.\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    os.environ[\"PYTHONWARNINGS\"] = \"ignore\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the data\n",
    "We fetch the Banana data from OpenML: https://www.openml.org/d/1460  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bananas = oml.datasets.get_dataset(1460) # Banana data has OpenML ID 1460\n",
    "X, y, _, _ = bananas.get_data(target=bananas.default_target_attribute, dataset_format='array');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0], X[:,1], c=y,cmap=plt.cm.bwr, marker='.');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting helpers. Based loosely on https://github.com/amueller/mglearn\n",
    "def plot_svm_kernel(X, y, title, support_vectors, decision_function, dual_coef=None, show=True):\n",
    "    \"\"\"\n",
    "    Visualizes the SVM model given the various outputs. It plots:\n",
    "    * All the data point, color coded by class: blue or red\n",
    "    * The support vectors, indicated by circling the points with a black border. \n",
    "      If the dual coefficients are known (only for kernel SVMs) if paints support vectors with high coefficients darker\n",
    "    * The decision function as a blue-to-red gradient. It is white where the decision function is near 0.\n",
    "    * The decision boundary as a full line, and the SVM margins (-1 and +1 values) as a dashed line\n",
    "    \n",
    "    Attributes:\n",
    "    X -- The training data\n",
    "    y -- The correct labels\n",
    "    title -- The plot title\n",
    "    support_vectors -- the list of the coordinates of the support vectores\n",
    "    decision_function - The decision function returned by the SVM\n",
    "    dual_coef -- The dual coefficients of all the support vectors (not relevant for LinearSVM)\n",
    "    show -- whether to plot the figure already or not\n",
    "    \"\"\"\n",
    "    # plot the line, the points, and the nearest vectors to the plane\n",
    "    #plt.figure(fignum, figsize=(5, 5))\n",
    "    plt.title(title)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, zorder=10, cmap=plt.cm.bwr, marker='.')\n",
    "    if dual_coef is not None:\n",
    "        plt.scatter(support_vectors[:, 0], support_vectors[:, 1], c=dual_coef[0, :],\n",
    "                    s=70, edgecolors='k', zorder=10, marker='.', cmap=plt.cm.bwr)\n",
    "    else:\n",
    "        plt.scatter(support_vectors[:, 0], support_vectors[:, 1], facecolors='none',\n",
    "                    s=70, edgecolors='k', zorder=10, marker='.', cmap=plt.cm.bwr)\n",
    "    plt.axis('tight')\n",
    "    x_min, x_max = -3.5, 3.5\n",
    "    y_min, y_max = -3.5, 3.5\n",
    "\n",
    "    XX, YY = np.mgrid[x_min:x_max:300j, y_min:y_max:300j]\n",
    "    Z = decision_function(np.c_[XX.ravel(), YY.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(XX.shape)\n",
    "    plt.contour(XX, YY, Z, colors=['k', 'k', 'k'], linestyles=['--', '-', '--'],\n",
    "                levels=[-1, 0, 1])\n",
    "    plt.pcolormesh(XX, YY, Z, vmin=-1, vmax=1, cmap=plt.cm.bwr, alpha=0.1)\n",
    "\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    \n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "    \n",
    "def heatmap(values, xlabel, ylabel, xticklabels, yticklabels, cmap=None,\n",
    "            vmin=None, vmax=None, ax=None, fmt=\"%0.2f\"):\n",
    "    \"\"\"\n",
    "    Visualizes the results of a grid search with two hyperparameters as a heatmap.\n",
    "    Attributes:\n",
    "    values -- The test scores\n",
    "    xlabel -- The name of hyperparameter 1\n",
    "    ylabel -- The name of hyperparameter 2\n",
    "    xticklabels -- The values of hyperparameter 1\n",
    "    yticklabels -- The values of hyperparameter 2\n",
    "    cmap -- The matplotlib color map\n",
    "    vmin -- the minimum value\n",
    "    vmax -- the maximum value\n",
    "    ax -- The figure axes to plot on\n",
    "    fmt -- formatting of the score values\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    # plot the mean cross-validation scores\n",
    "    img = ax.pcolor(values, cmap=cmap, vmin=None, vmax=None)\n",
    "    img.update_scalarmappable()\n",
    "    ax.set_xlabel(xlabel, fontsize=10)\n",
    "    ax.set_ylabel(ylabel, fontsize=10)\n",
    "    ax.set_xticks(np.arange(len(xticklabels)) + .5)\n",
    "    ax.set_yticks(np.arange(len(yticklabels)) + .5)\n",
    "    ax.set_xticklabels(xticklabels)\n",
    "    ax.set_yticklabels(yticklabels)\n",
    "    ax.set_aspect(1)\n",
    "    \n",
    "    ax.tick_params(axis='y', labelsize=12)\n",
    "    ax.tick_params(axis='x', labelsize=12, labelrotation=90)\n",
    "\n",
    "    # Add annotations\n",
    "    for i in range(values.shape[0]):\n",
    "        for j in range(values.shape[1]):\n",
    "            value = values[i, j]  # Get the scalar value\n",
    "            x = j + 0.5  # Center of the cell\n",
    "            y = i + 0.5  # Center of the cell\n",
    "            color = img.get_facecolor()[i * values.shape[1] + j]\n",
    "            if np.mean(color[:3]) > 0.5:\n",
    "                c = 'k'  # Black text for light background\n",
    "            else:\n",
    "                c = 'w'  # White text for dark background\n",
    "            ax.text(x, y, fmt % value, color=c, ha=\"center\", va=\"center\", size=10)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Linear SVMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll look at linear SVMs and the different outputs they produce. Check the [documentation of LinearSVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC)\n",
    "\n",
    "The most important inputs are:  \n",
    "* C -- The C hyperparameter controls the misclassification cost and therefore the amount of regularization. Lower values correspond to more regularization\n",
    "* loss - The loss function, typically 'hinge' or 'squared_hinge'. Squared hinge is the default. Normal hinge is less strict.\n",
    "* dual -- Whether to solve the primal optimization problem or the dual (default). The primal is recommended if you have many more data points than features (although our datasets is very small, so it won't matter much).\n",
    "\n",
    "The most important outputs are:  \n",
    "* decision_function - The function used to classify any point. In this case on linear SVMs, this corresponds to the learned hyperplane, or $y = \\mathbf{wX} + b$. It can be evaluated at every point, if the result is positive the point is classified as the positive class and vice versa. \n",
    "* coef_ - The model coefficients, i.e. the weights $\\mathbf{w}$\n",
    "* intercept_ - the bias $b$\n",
    "\n",
    "From the decision function we can find which points are support vectors and which are not: the support vectors are all\n",
    "the points that fall inside the margin, i.e. have a decision value between -1 and 1, or that are misclassified. Also see the lecture slides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1: Linear SVMs\n",
    "Train a LinearSVC with C=0.001 and hinge loss. Then, use the plotting function `plot_svm_kernel` to plot the results. For this you need to extract the support vectors from the decision function. There is a hint below should you get stuck. \n",
    "Interpret the plot as detailed as you can. Afterwards you can also try some different settings. You can also try using the primal instead of the dual optimization problem (in that case, use squared hinge loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: how to compute the support vectors from the decision function (ignore if you want to solve this yourself)\n",
    "# support_vector_indices = np.where((2 * y - 1) * clf.decision_function(X) <= 1)[0]\n",
    "# support_vectors = X[support_vector_indices]\n",
    "\n",
    "# Note that we can also calculate the decision function manually with the formula y = w*X\n",
    "# decision_function = np.dot(X, clf.coef_[0]) + clf.intercept_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.dpi'] = 200 # Make figures a bit bigger\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "clf1_1 = LinearSVC(C=0.001, loss='hinge')\n",
    "clf1_1.fit(X, y)\n",
    "\n",
    "support_vector_indices = np.where((2 * y - 1) * clf1_1.decision_function(X) <= 1)[0]\n",
    "support_vectors = X[support_vector_indices]\n",
    "\n",
    "plot_svm_kernel(X, y, \"Linear SVM\",support_vectors,clf1_1.decision_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation: As expected, the data cannot be fitted well with a linear SVM. Almost all points fall within the margin. Almost all points are support vectors, except for a few blue points on the top right. The accuracy is only 57%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Kernelized SVMs\n",
    "\n",
    "Check the [documentation of SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)\n",
    "\n",
    "It has a few more inputs. The most important: \n",
    "* kernel - It must be either ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, or your custom defined kernel.\n",
    "* gamma - The kernel width of the `rbf` (Gaussian) kernel. Smaller values mean wider kernels. \n",
    "          Only relevant when selecting the rbf kernel.\n",
    "* degree - The degree of the polynomial kernel. Only relevant when selecting the poly kernel.\n",
    "\n",
    "There also also more outputs that make our lifes easier:  \n",
    "* support_vectors_ - The array of support vectors\n",
    "* n_support_ - The number of support vectors per class\n",
    "* dual_coef_ - The coefficients of the support vectors (the dual coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1\n",
    "\n",
    "Evaluate different kernels, with their default hyperparameter settings.\n",
    "Outputs should be the 5-fold cross validated accuracy scores for the linear kernel (lin_scores), polynomial kernel (poly_scores) and RBF kernel (rbf_scores). Print the mean and variance of the scores and give an initial interpretation of the performance of each kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Linear kernel\n",
    "clf = svm.SVC(kernel='linear')\n",
    "lin_scores = cross_val_score(clf, X, y)\n",
    "\n",
    "# Polynomial kernel\n",
    "clf = svm.SVC(kernel='poly')\n",
    "poly_scores = cross_val_score(clf, X, y)\n",
    "\n",
    "# RBF kernel\n",
    "clf = svm.SVC(kernel='rbf')\n",
    "rbf_scores = cross_val_score(clf, X, y)\n",
    "\n",
    "print(\"AUC Linear Kernel: {:.2f} +- {:.4f}\".format(lin_scores.mean(), np.var(lin_scores)))\n",
    "print(\"AUC Polynomial Kernel: {:.2f} +- {:.4f}\".format(poly_scores.mean(), np.var(poly_scores)))\n",
    "print(\"AUC RBF Kernel: {:.2f} +- {:.5f}\".format(rbf_scores.mean(), np.var(rbf_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear kernel has a very low score, and is likely underfitting severely. \n",
    "The polynomial kernel does a lot better. The RBF kernel works particularly well, even without any tuning. \n",
    "The models are very stable, there is hardly any variance in the scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Visualizing the fit\n",
    "To better understand what the different kernels are doing, let's visualize their predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1\n",
    "Call and fit SVM with linear, polynomial and RBF kernels with default parameter values. For RBF kernel, use kernel coefficient value (gamma) of 2.0. Plot the results for each kernel with \"plot_svm_kernel\" function. The plots show the predictions made for the different kernels. The background color shows the prediction (blue or red). The full line shows the decision boundary, and the dashed line the margin. The encircled points are the support vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.dpi'] = 120 # Make figures a bit bigger\n",
    "\n",
    "#Linear\n",
    "clf1 = svm.SVC(kernel='linear', C=1.0, tol=1e-3)  #default values for parameters\n",
    "clf1.fit(X, y)\n",
    "plot_svm_kernel(X,y,\"Linear kernel\", \n",
    "                clf1.support_vectors_, \n",
    "                clf1.decision_function, clf1.dual_coef_)\n",
    "\n",
    "#Polynomial\n",
    "clf2 = svm.SVC(kernel='poly', C=1.0)\n",
    "clf2.fit(X, y)\n",
    "plot_svm_kernel(X,y,\"Polynomial kernel\", \n",
    "                clf2.support_vectors_, \n",
    "                clf2.decision_function, clf2.dual_coef_)\n",
    "\n",
    "#RBF\n",
    "clf3 = svm.SVC(kernel='rbf', gamma=2, C=1.0)\n",
    "clf3.fit(X,y)\n",
    "plot_svm_kernel(X,y,\"RBF kernel\", \n",
    "                clf3.support_vectors_, \n",
    "                clf3.decision_function, clf3.dual_coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2\n",
    "Interpret the plots for each kernel. Think of ways to improve the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear**: It's clear that this data is not linearly separable. The linear SVM is badly underfitting. There also appear to be some optimization issues, as the decision boundary lies way outside of the image, and there is a group of non-support vectors that should be support vectors. Forcing more optimization (by decreasing tolerance of the stopping criterion `tol`) yields slightly better results, but will also slow down the optimization (try it of you like).\n",
    "\n",
    "**Polynomial**: A slightly better fit, but clearly polynomials aren't the best fit either. They divide the space in subspaces that don't capture the banana shapes at all.\n",
    "\n",
    "**RBF**: Works very nicely, and the default settings seem to actually hit the sweet spot. We should still try to tune C and gamma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Visualizing the RBF models and hyperparameter space\n",
    "Select the RBF kernel and optimize the two most important hyperparameters (the 𝐶 parameter and the kernel width 𝛾 ).\n",
    "\n",
    "Hint: values for C and $\\gamma$ are typically in [$2^{-15}..2^{15}$] on a log scale. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.1\n",
    "First try 3 very different values for $C$ and $\\gamma$ (for instance [1e-3,1,1e3]). For each of the 9 combinations, create the same RBF plot as before to understand what the model is doing. Also create a standard train-test split and report the train and test score. Explain the performance results. When are you over/underfitting? Can you see this in the predictions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For convenience we'll plot the results in a 3x3 grid\n",
    "plt.figure(figsize=(15, 10))\n",
    "fig_num = 0\n",
    "\n",
    "# build a standard stratified train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, stratify=y, random_state=0)\n",
    "\n",
    "for c in [0.001, 1, 1000]:\n",
    "    for gamma in [0.001, 1, 1000]:\n",
    "        fig_num += 1\n",
    "        plt.subplot(3, 3, fig_num) # plot in a 3x3 grid\n",
    "        clf4 = svm.SVC(kernel='rbf',C=float(c),gamma=float(gamma)) # setup and fit the model\n",
    "        clf4.fit(X_train, y_train)\n",
    "        plot_svm_kernel(X_train,y_train,\"C={}, g={}, trainACC {:.2f}, testACC {:.2f}\".format(c,gamma,clf4.score(X_train,y_train), clf4.score(X_test,y_test)), \n",
    "                clf4.support_vectors_, clf4.decision_function, clf4.dual_coef_, show=False)\n",
    "        \n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For C = 0.001 (top row), the SVM is always underfitting. The boundaries look very different but all are underfitting because they are over-regularized.\n",
    "- For gamma = 1000 (narrow Gaussians, right column), almost all datapoints are support vectors, For higher values of C, they are clearly overfitting: the decision boundaries are islands around each point, the model predicts 0 everywhere else. \n",
    "- The best results are found for medium C, medium gamma. This also yields the fewest support vectors. The decision boundaries show that it captures the banana shapes well.\n",
    "- Large C values (bottom row) tend to cause more overfitting unless gamma is very small. These two types of regularization clearly interact with each other.\n",
    "- For gamma=1, you can also see that the margins for C=1000 are much more narrow than those for C=1. Although not visible in the scores, it is clear that the center model (with the larger margins) will generalize better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.2\n",
    "Optimize the hyperparameters using a grid search, trying every possible combination of C and gamma. Show a heatmap of the results and report the optimal hyperparameter values. Use at least 10 values for $C$ and $\\gamma$ in [$2^{-15}..2^{15}$] on a log scale. Report accuracy under 3-fold CV. We recommend to use sklearn's [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) and the `heatmap` function defined above. Check their documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define and fit a grid search with 3-fold cross validation for SVM - RBF kernel. \n",
    "# Quite a detailed grid search. May take a while.\n",
    "svc = svm.SVC(kernel='rbf')\n",
    "resolution = 25\n",
    "param_grid = {'C': np.logspace(-12,12,resolution,base=2),\n",
    "              'gamma': np.logspace(-12,12,resolution,base=2)}\n",
    "grid_search = GridSearchCV(svc, param_grid, cv=3, n_jobs=-1, scoring='accuracy');\n",
    "grid_search.fit(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot with heatmap\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "scores = np.array(results.mean_test_score).reshape(resolution, resolution)\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "fig, axes = plt.subplots(1, 1, figsize=(13, 13))\n",
    "heatmap(scores, xlabel='gamma', xticklabels=np.around(param_grid['gamma'],4),\n",
    "                      ylabel='C', yticklabels=np.around(param_grid['C'],4), cmap=\"viridis\", fmt=\"%.2f\", ax=axes);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Supposons que vous avez déjà exécuté le GridSearchCV et obtenu les résultats\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "scores = np.array(results.mean_test_score).reshape(resolution, resolution)\n",
    "\n",
    "# Mise à jour des paramètres de la police\n",
    "plt.rcParams.update({'font.size': 8})\n",
    "\n",
    "# Création de la heatmap avec seaborn\n",
    "fig, axes = plt.subplots(1, 1, figsize=(13, 13))\n",
    "sns.heatmap(scores, \n",
    "            xticklabels=np.around(param_grid['gamma'], 4),\n",
    "            yticklabels=np.around(param_grid['C'][::-1], 4),\n",
    "            cmap=\"viridis\", \n",
    "            annot=True, \n",
    "            fmt=\".2f\", \n",
    "            ax=axes)\n",
    "\n",
    "axes.set_xlabel('gamma')\n",
    "axes.set_ylabel('C')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there isn't really an simple optimal peak in the C-gamma space, but rather a 'ridge' of optimal performance. For instance, (gamma=0.25, C=4096) has top performance, \n",
    "but so does (gamma=2.0, C=0.25)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
